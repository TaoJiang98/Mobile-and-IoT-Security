{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "call classifying model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znBYhtREuJRu",
        "outputId": "7ae7f2b9-0321-4bc0-ed9a-b31c0cc3a5fd"
      },
      "source": [
        "!unzip fraudcalls.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  fraudcalls.zip\n",
            "  inflating: fraud_call.file         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5lZkG2E9NSr",
        "outputId": "73ed5dd3-154b-4d6c-dc52-e47613f6d852"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "def loadData(file):\n",
        "    text, labels = [], []\n",
        "    with open(file, 'r') as data: \n",
        "        for line in data:\n",
        "            split = line.strip().split('\\t')\n",
        "            if (split[0] != \"\"):\n",
        "                labels.append(split[0])\n",
        "                text.append(split[1])\n",
        "\n",
        "    return text, labels\n",
        "\n",
        "alltext, alllabels = loadData(\"fraud_call.file\")\n",
        "\n",
        "norm = 0\n",
        "fraud  = 0\n",
        "\n",
        "for i in range(0,len(alllabels)):\n",
        "    if (alllabels[i] == \"fraud\"):\n",
        "        fraud+=1\n",
        "    if (alllabels[i] == \"normal\"):\n",
        "        norm+=1\n",
        "\n",
        "print(len(alllabels))\n",
        "print(norm)\n",
        "print(fraud)\n",
        "\n",
        "print(math.floor(norm*.65))\n",
        "print(math.floor(fraud*.65))\n",
        "print(math.floor(norm*.2))\n",
        "print(math.floor(fraud*.2))\n",
        "print(math.floor(norm*.15))\n",
        "print(math.floor(fraud*.15))\n",
        "\n",
        "l = np.array(alllabels)\n",
        "t = np.array(alltext)\n",
        "\n",
        "wholedataset = np.column_stack((l,t))\n",
        "np.random.shuffle(wholedataset)\n",
        "\n",
        "#gonna turn these into numpy arrays\n",
        "train = []  \n",
        "verify = []\n",
        "test = []\n",
        "\n",
        "i=0 \n",
        "while (len(train) < 3437):\n",
        "    if (wholedataset[i][0] == \"normal\"):\n",
        "        train.append(wholedataset[i])\n",
        "    i+=1\n",
        "print(i)\n",
        "while (len(verify) < 1057):\n",
        "    if (wholedataset[i][0] == \"normal\"):\n",
        "        verify.append(wholedataset[i])\n",
        "    i+=1\n",
        "while (len(test) < 795):\n",
        "    if (wholedataset[i][0] == \"normal\"):\n",
        "        test.append(wholedataset[i])\n",
        "    i+=1\n",
        "i=0\n",
        "while (len(train) < 3437 + 414):\n",
        "    if (wholedataset[i][0] == \"fraud\"):\n",
        "        train.append(wholedataset[i])\n",
        "    i+=1\n",
        "while (len(verify) < 1057+127):\n",
        "    if (wholedataset[i][0] == \"fraud\"):\n",
        "        verify.append(wholedataset[i])\n",
        "    i+=1\n",
        "while (len(test) < 795+97):\n",
        "    if (wholedataset[i][0] == \"fraud\"):\n",
        "        test.append(wholedataset[i])\n",
        "    i+=1\n",
        "\n",
        "nptrain = np.stack(train, axis=0)\n",
        "npverify = np.stack(verify, axis=0)\n",
        "nptest = np.stack(test, axis=0)\n",
        "\n",
        "np.random.shuffle(nptrain)\n",
        "np.random.shuffle(npverify)\n",
        "np.random.shuffle(nptest)\n",
        "\n",
        "print(nptrain[:5])\n",
        "print(npverify[:5])\n",
        "print(nptest[:5]) "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5927\n",
            "5289\n",
            "638\n",
            "3437\n",
            "414\n",
            "1057\n",
            "127\n",
            "793\n",
            "95\n",
            "3856\n",
            "[['normal' 'Sure. I hope your complaints will have gone till then.']\n",
            " ['normal'\n",
            "  '\"Life is nothing wen v get everything\". But \"life is everything wen v miss something \". Real value of people wil be realized only in their absence.... gud mrng']\n",
            " ['normal'\n",
            "  \"Oh, the grand is having a bit of a party but it doesn't mention any cover charge so it's probably first come first served\"]\n",
            " ['normal' 'Cool. I am  &lt;#&gt;  inches long. hope you like them big!']\n",
            " ['normal' 'I am on the way to ur home']]\n",
            "[['normal'\n",
            "  \"cThen i thk shd b enuff.. Still got conclusion n contents pg n references.. I'll b doing da contents pg n cover pg..\"]\n",
            " ['normal' 'What class of  &lt;#&gt;  reunion?']\n",
            " ['normal'\n",
            "  'I have 2 sleeping bags, 1 blanket and paper and  phone details. Anything else?']\n",
            " ['normal' 'Y lei?']\n",
            " ['normal' 'will you like to be spoiled? :)']]\n",
            "[['normal' 'Dunno why you ask me.']\n",
            " ['normal'\n",
            "  'I have been so busy with my new job that I have not had the time to do much else but otherwise, me and the family are all fine.']\n",
            " ['normal'\n",
            "  \"Let me know how to contact you. I've you settled in a room. Lets know you are ok.\"]\n",
            " ['normal' 'Are you being good, baby? :)']\n",
            " ['normal'\n",
            "  'Its posible dnt live in  &lt;#&gt; century cm frwd n thnk different']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo63828oov4E"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "traindata = vectorizer.fit_transform(nptrain[:,1])\n",
        "valdata = vectorizer.transform(npverify[:,1])\n",
        "testdata = vectorizer.transform(nptest[:,1])\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-eiJoQIzsxy",
        "outputId": "f9f2d8d2-2ce5-4a74-c682-59aeff71969b"
      },
      "source": [
        "classifier = LogisticRegression(penalty='l2', fit_intercept=True, C=1)\n",
        "\n",
        "classifier.fit(traindata, nptrain[:,0])\n",
        "print(\"training accuracy = \", classifier.score(traindata, nptrain[:,0]))\n",
        "print(\"validation accuracy = \", classifier.score(valdata, npverify[:,0]))\n",
        "\n",
        "classifier.predict(vectorizer.transform([\"this is a phone call where i commit insurance fraud\"]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training accuracy =  0.9968839262529213\n",
            "validation accuracy =  0.9763513513513513\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['normal'], dtype='<U910')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6dcvhT7L81w"
      },
      "source": [
        "import pickle \n",
        "\n",
        "modelname = \"logregmodel.sav\"\n",
        "pickle.dump(classifier, open(modelname, \"wb\"))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrikW87ZkmHs"
      },
      "source": [
        "#need the vectorizer too!\n",
        "\n",
        "pickle.dump(vectorizer, open(\"vectorizer.sav\", \"wb\"))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhU5IorFIFdX",
        "outputId": "d946131b-e402-4f56-e0e3-2bbeb172c941"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "numfraud = 0\n",
        "for i in range(0,len(npverify)):\n",
        "    if npverify[i][0] == \"fraud\":\n",
        "        numfraud +=1\n",
        "\n",
        "print(numfraud)\n",
        "\n",
        "valpreds = classifier.predict(valdata)\n",
        "confusion = confusion_matrix(npverify[:,0],valpreds)\n",
        "\n",
        "#19 false negatives\n",
        "print(confusion)\n",
        "precision = confusion[0][0] / (confusion[0][0] + confusion[1][0])\n",
        "recall = confusion[0][0] / (confusion[0][0] + confusion[0][1])\n",
        "f1s =  2 * ((recall * precision)/(recall + precision))\n",
        "print(precision)\n",
        "print(recall)\n",
        "print(f1s)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127\n",
            "[[ 102   25]\n",
            " [   3 1054]]\n",
            "0.9714285714285714\n",
            "0.8031496062992126\n",
            "0.8793103448275863\n"
          ]
        }
      ]
    }
  ]
}